{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HTC-Grid","text":"<p>The high throughput compute grid project (HTC-Grid) is a container based cloud native HPC/Grid environment. The project provides a reference architecture that can be used to build and adapt a modern High throughput compute solution using underlying AWS services, allowing users to submit high volumes of short and long running tasks and scaling environments dynamically.</p> <p>Warning: This project is an Open Source (Apache 2.0 License), not a supported AWS Service offering.</p>"},{"location":"#when-should-i-use-htc-grid","title":"When should I use HTC-Grid ?","text":"<p>HTC-Grid should be used when the following criteria are meet:</p> <ul> <li>high task throughput is required (from 250 to 10,000+ tasks per second).</li> <li>The tasks are loosely coupled. </li> <li>Variable workloads (tasks with heterogeneous execution times) are expected and the solution needs to dynamically scale with the load.</li> </ul>"},{"location":"#when-should-i-not-use-the-htc-grid","title":"When should I not use the HTC-Grid ?","text":"<p>HTC-Grid might not be the best choice if :</p> <ul> <li>The required task throughput is below 250 tasks per second: Use AWS Batch instead. </li> <li>The tasks are tightly coupled, or use MPI. Consider using either AWS Parallel Cluster or AWS Batch Multi-Node workloads instead </li> <li>The tasks uses third party licensed software.</li> </ul>"},{"location":"#how-do-i-use-htc-grid","title":"How do I use HTC-Grid ?","text":"<p>If you want to use the HTC-Grid, please visit the following pages:</p> <ul> <li>Getting Started</li> <li>User Guide</li> <li>Developer Guide </li> <li>The workshop</li> </ul>"},{"location":"api/","title":"API documentation","text":"<p>This section outlines how to develop and deploy a custom application on HTC-Grid. At the top level, there are 3 main components that need to be developed:</p> <ol> <li>A client application(s) that will interact with a deployment of HTC-Grid by submitting tasks and retrieving results.</li> <li>A worker (lambda) function that will be receiving and executing tasks.</li> <li>Configuration of the HTC-Grid's deployment process to incorporate all the relevant changes (specifically for the backhand worker functions).</li> </ol>"},{"location":"api/client/","title":"Client API (between application and HTC-Grid)","text":"<p>HTC-Grid provides a Python3 API (please refer to API Reference for more details) with several client application examples of how to use the API (see <code>client.py</code>, <code>cancel_tasks.py</code>, and <code>portfolio_pricing_client.py</code>  examples in <code>/examples/client/python</code>).</p> <p>These examples show how to: - Connect to an HTC-Grid deployment - Authenticate the client - Submit multiple tasks - Wait for the results - Retrieve results</p> <p>Follow these examples to write a custom applications.</p>"},{"location":"api/client/python/reference/","title":"HTC-Grid API Reference","text":""},{"location":"api/client/python/reference/#client-api-python","title":"Client API (Python)","text":"<p>This section outlines how client application can connect and interact with a deployed HTC-Grid.</p> <p>AWSConnector is the main class responsible for communication with the HTC-Grid. Upon creating the</p> <p><pre><code>class api.connector.AWSConnector\n\nfrom api.connector import AWSConnector\n</code></pre> These are the available methods:</p> <ul> <li>AWSConnector</li> <li>authenticate</li> <li>send</li> <li>get_results</li> <li>cancel_sessions</li> </ul>"},{"location":"api/client/python/reference/#constructor-awsconnector","title":"Constructor - <code>AWSConnector</code>","text":"<p>Request Syntax</p> <pre><code>gridConnector = AWSConnector(client_config_data=\n  {\n      'grid_storage_service' : 'REDIS'|'S3',\n      's3_bucket' : 'string',\n      'redis_url' :'string',\n      'public_api_gateway_url' : 'string',\n      'private_api_gateway_url' : 'string',\n      'api_gateway_key' : 'string',\n      'user_pool_id' : 'string',\n      'cognito_userpool_client_id' : 'string',\n      'username' : 'string',\n      'password' : 'string',\n      'dynamodb_results_pull_interval_sec' : 'number',\n      'task_input_passed_via_external_storage' : 'number',\n      'region' : 'string'\n  }\n)\n</code></pre> <p>Parameters client_config_data (dict) [REQUIRED]</p> <ul> <li><code>grid_storage_service</code> - Determines which storage will be used for Data Plane 'REDIS'|'S3'</li> <li><code>s3_bucket</code> - The name of the S3 bucket that is used as a back-end for Data Plane</li> <li><code>redis_url</code> - The URL of the Redis deployment that is used as a back-end for the Data Plane</li> <li><code>public_api_gateway_url</code> -</li> <li><code>private_api_gateway_url</code> -</li> <li><code>api_gateway_key</code> -</li> <li><code>user_pool_id</code> -</li> <li><code>cognito_userpool_client_id</code> -</li> <li><code>username</code> - (optional) username for Cognito userpool, if the field is not present, <code>username</code> property is read from environment variable <code>USERNAME</code></li> <li><code>password</code> - (optional) password for Cognito userpool, if the field is not present, <code>password</code> property is read from environment variable <code>PASSWORD</code></li> <li><code>dynamodb_results_pull_interval_sec</code> - The frequency that the client uses to fetch results from DynamoDB.</li> <li><code>REGION</code> - Region where HTC-Grid is deployed</li> </ul> <p>Return type AWSConnector Object</p>"},{"location":"api/client/python/reference/#method-authenticate","title":"Method - <code>authenticate</code>","text":"<p>There are currently three ways to authenticate a client. 1. Passing <code>username</code> and <code>password</code> via <code>client_config_data</code> when initializing AWSConnector (not recommended). 2. Setting <code>username</code> and <code>password</code> in the environmental variables 3. If client application and HTC-Grid are located in the same VPN then there is no need for explicit authentication. However, an additional environmental variable needs to be set <code>INTRA_VPC=1</code> allowing AWSConnector to skip username and password.</p> <p>Request Syntax</p> <pre><code>gridConnector.authenticate()\n</code></pre> <p>Parameters</p> <p>None</p> <p>Return type</p> <p>None</p> <p>Returns</p> <p>None</p>"},{"location":"api/client/python/reference/#method-send","title":"Method - <code>send</code>","text":"<p>This function is used to send task(s) to the HTC-Grid.</p> <p>Request Syntax</p> <pre><code>gridConnector.send(tasks_list=[\n   {},\n   ]\n)\n</code></pre> <p>Parameters tasks_list (list) [REQUIRED]</p> <p>A list of serializable dictionaries. Each dictionary will be passed to the execution lambda function as an event argument. Each dictionary will be encoded to base 64 before being stored in the Data Plane and then decoded before being passed to the execution lambda function. Output produced by the lambda function will be passed the same way in the reverse direction. Encoding and decoding is done by the grid connector, client only needs to provide serializable dictionary as input and output of the lambda functions.</p> <pre><code>input:\nbase64.b64encode(json.dumps(input_dict).encode('utf-8'))\n\noutput:\nbase64.b64decode(output_dict).decode('utf-8')\n</code></pre> <p>Return type</p> <p>Dict</p> <p>Returns</p> <p>On successful submission, function returns a dictionary.</p> <pre><code>{\n   'session_id': 'string',\n   'task_ids': [\n      'string',\n   ],\n}\n</code></pre> <ul> <li><code>session_id</code> - a single session ID that is associated with the submission.</li> <li><code>task_ids</code> - an ordered list of task IDs associated with each task that was submitted in the request.</li> </ul>"},{"location":"api/client/python/reference/#method-get_results","title":"Method - <code>get_results</code>","text":"<p>Blocking function, waits until all tasks in the session are completed or until the timeout is expired. Function returns task IDs that have reached their terminal state (i.e., their states will not change). - Note, function does not return outputs of the lambdas, it is responsibility of the client to retrieve results from the Data Plane asynchronously. This function merely indicates that tasks are completed and results can be retrieved from the Data Plane.</p> <p>Request Syntax</p> <pre><code>gridConnector.get_results(\n   submission_response = {\n      'session_id' : 'string',\n      'task_ids': [\n         'string',\n      ],\n   }\n   timeout_sec = 'number'\n)\n</code></pre> <p>Parameters</p> <ul> <li><code>submission_response</code> - a dictionary that was returned after successful submission of tasks. <code>submission_response</code> must contain a valid <code>session_id</code> and a list of associated <code>task_ids</code>.</li> <li><code>timeout_sec</code> - the function will periodically (based on <code>dynamodb_results_pull_interval_sec</code>) will try to pull for results until all tasks in the session are reached their terminal states or until the timeout is expires. The function uses the length of the <code>task_ids</code> list to determine the number of expected responses from the grid.</li> </ul> <p>Return type</p> <p>Dict</p> <p>Returns</p> <p><pre><code>{\n   'finished': [\n      'string',\n   ]\n   'finished_OUTPUT': [\n      'string',\n   ],\n   'failed': [\n      'string',\n   ]\n   'failed_OUTPUT': [\n      'string',\n   ],\n   'cancelled': [\n      'string',\n   ]\n   'cancelled_OUTPUT': [\n      'string',\n   ],\n   'metadata': {\n      'tasks_in_response': 'number'\n   }\n}\n</code></pre> * <code>finished</code> (optional) - list of finished task IDs * <code>finished_OUTPUT</code> (optional) - returns a string output produced by the lambda function * <code>cancelled</code> (optional) list of canceled task IDs * <code>cancelled_OUTPUT</code> (optional) - returns a hard-coded string <code>read_from_REDIS</code> indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * <code>failed</code> (optional) list of failed task IDs * <code>failed_OUTPUT</code> (optional) - returns a hard-coded string <code>read_from_REDIS</code> indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * <code>metadata</code>    * <code>tasks_in_response</code> - total number of task in the terminal state (finished + failed + canceled) returned to the response. For example, if none of the tasks in the session have reached their terminal states an expected return is as follows:    <pre><code>{\n   'metadata': {\n      'tasks_in_response': 0\n   }\n}\n</code></pre></p>"},{"location":"api/client/python/reference/#method-cancel_sessions","title":"Method - <code>cancel_sessions</code>","text":"<p>Request Syntax</p> <pre><code>response = gridConnector.cancel_sessions(\n   [\n      'string',\n   ]\n)\n</code></pre> <p>Parameters</p> <ul> <li>Function takes list of session IDs to be canceled</li> </ul> <p>Return type</p> <p>Dict</p> <p>Returns</p> <p>Function returns dictionary of processed session IDs.</p> <pre><code>{\n   'string':\n   {\n      'cancelled_retrying': 0,\n      'cancelled_pending': 3,\n      'cancelled_processing': 1,\n      'total_cancelled_tasks': 4\n   },\n\n   'string': {....}, #session - 2\n   'string': {....}, #session - 3\n}\n</code></pre> <ul> <li><code>string</code> - keys of the response dictionary are session IDs that were passed in for cancellation. Sub-dictionaries contain information about how many tasks were moved into canceled state.</li> <li><code>cancelled_retying</code> - number of tasks moved from retrying state into canceled state</li> <li><code>cancelled_pending</code> - number of tasks moved from pending state into canceled state</li> <li><code>cancelled_processing</code> - number of tasks moved from the processing state into canceled state.</li> <li><code>total_cancelled_tasks</code> - total number of tasks that has been affected by this invocation.</li> </ul>"},{"location":"api/client/python/usage/","title":"How to use API in your application","text":"<p>This section outlines how to integrate the HTC grid with your current application.</p>"},{"location":"api/client/python/usage/#how-to-use-the-api-in-your-application","title":"How to use the API in your application","text":"<p>Below you will find a python example that demonstrate how to use the API in your application <pre><code>from api.connector import AWSConnector\n\nimport os\nimport json\nimport logging\n\nclient_config_file = os.environ['AGENT_CONFIG_FILE']\n\nwith open(client_config_file, 'r') as file:\n    client_config_file = json.loads(file.read())\n\n\nif __name__ == \"__main__\":\n\n    logging.info(\"Simple Client\")\n    gridConnector = AWSConnector()\n\n    gridConnector.init(client_config_file, username=username, password=password)\n    gridConnector.authenticate()\n\n    task_1_definition = {\n        \"worker_arguments\": [\"1000\", \"1\", \"1\"]\n    }\n\n    task_2_definition = {\n        \"worker_arguments\": [\"2000\", \"1\", \"1\"]\n    }\n\n    submission_resp = gridConnector.send([task_1_definition, task_2_definition])\n    logging.info(submission_resp)\n\n\n    results = gridConnector.get_results(submission_resp, timeout_sec=100)\n    logging.info(results)\n</code></pre></p> <p>Current release of HTC-Grid includes only Python3 API. However, if required, it is possible to develop a custom API using different languages (e.g., Java, .Net, etc.). Existing API is very concise and relies on the AWS API to interact with AWS services (refer to./source/client/python/api-v0.1/api for the example).</p>"},{"location":"api/client/python/usage/#how-to-run-a-client-application","title":"How to run a client application","text":""},{"location":"api/client/python/usage/#running-a-client-application-as-a-pod-on-eks","title":"Running a Client Application as a pod on EKS","text":"<p>This is the easiest way to deploy a client application for testing purposes. Overview: 1. A client application is being packaged locally into a container. 2. The container is being deployed on the same EKS cluster governed by HTC-Grid as Job. 3. Once container is deployed, it launches the client application that submits the tasks.</p>"},{"location":"api/client/python/usage/#_1","title":"How to use API in your application","text":"<p>Details: 1. Build and Push docker image Build an image that will have all dependencies to be able to execute the client. See example in <code>examples/submissions/k8s_jobs/Dockerfile.Submitter</code>.    In the example below we copy client.py into a container and install all dependencies form the requirements.txt.</p> <pre><code>```Docker\nFROM python:3.7.7-slim-buster\n\nRUN mkdir -p /app/py_connector\nRUN mkdir -p /dist\n\nCOPY ./dist/* /dist/\nCOPY ./examples/client/python/requirements.txt /app/py_connector/\n\nWORKDIR /app/py_connector\n\nRUN pip install -r requirements.txt\n\nCOPY ./examples/client/python/client.py .\n```\n</code></pre> <p>Push the image into the registry</p> <pre><code>```Makefile\nSUBMITTER_IMAGE_NAME=submitter\nTAG=&lt;the tag specified during the HTC-Grid deployment&gt;\nDOCKER_REGISTRY=$(ACCOUNT_ID).dkr.ecr.$(REGION).amazonaws.com\n\ndocker push $(DOCKER_REGISTRY)/$(SUBMITTER_IMAGE_NAME):$(TAG)\n```\n</code></pre> <ol> <li> <p>Create a deployment .yaml file. For example:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\nname: single-task\nspec:\ntemplate:\nspec:\ncontainers:\n- name: generator\nsecurityContext:\n{}\nimage: XXXXXX.dkr.ecr.eu-west-1.amazonaws.com/submitter:XXXX\nimagePullPolicy: Always\nresources:\nlimits:\ncpu: 100m\nmemory: 128Mi\nrequests:\ncpu: 100m\nmemory: 128Mi\ncommand: [\"python3\",\"./simple_client.py\", \"-n\", \"1\",  \"--worker_arguments\", \"1000 1 1\",\"--job_size\",\"1\",\"--job_batch_size\",\"1\",\"--log\",\"warning\"]\nvolumeMounts:\n- name: agent-config-volume\nmountPath: /etc/agent\nenv:\n- name: INTRA_VPC\nvalue: \"1\"\nrestartPolicy: Never\nnodeSelector:\ngrid/type: Operator\ntolerations:\n- effect: NoSchedule\nkey: grid/type\noperator: Equal\nvalue: Operator\nvolumes:\n- name: agent-config-volume\nconfigMap:\nname: agent-configmap\nbackoffLimit: 0\n</code></pre> </li> </ol> <p>For the examples provided with HTC-Grid, these .yaml files are generated automatically when <code>make happy-path</code> command is executed. Each .yaml file is a template (e.g., examples/submissions/k8s_jobs/single-task-test.yaml.tpl) that is being filled in with relevant fields that match the AWS account and deployment configuration. The substitution is done by examples/submissions/k8s_jobs/Makefile and relevant attributes are passed in by the ./Makefile.</p> <p>Note, once docker image is built and pushed into registry, the yaml file contains the <code>command</code> parameter that tells how to launch the client application once container is running.</p> <p>The deployment yaml file can be generated by extending existing build process, written manually, or generated in any other way that suited the workload.</p> <ol> <li> <p>To launch the client, simply execute the following command.</p> <pre><code>kubectl apply -f ./generated/&lt;custom.yaml.file&gt;.yaml\n</code></pre> </li> </ol>"},{"location":"api/client/python/usage/#running-a-client-application-locally","title":"Running a Client Application locally","text":"<p>"},{"location":"api/worker/","title":"Developing a Worker Function","text":"<p>HTC-Grid uses Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod running two containers: (i) an Agent and a (ii) Worker Function.</p> <ul> <li>The Agent provides a connectivity layer between the HTC-Grid and the Worker container. Agent's responsibilities include: (i) pulling for new tasks, (ii) interacting with the Data Plane (I/O), (iii) sending heartbeats back to Control Plane, and (iv) indicating completion of a task. Note, Agent does not need to be changed when developing new applications on HTC-Grid.</li> <li>The Worker container executes the custom code that performs the computational task. The execution is done locally within the container. The code of the worker function needs to be modified during the development.<ul> <li>Note: depending on the workload it is possible for the Worker function to access HTC-Grid's Data Plane directly or to access any other external systems as might be required. This functionality is not provided as part of the HTC-Grid.</li> </ul> </li> </ul> <p>At the high level the development process involves 4 steps:</p> <ol> <li>Write a custom Worker code for the target workload.</li> <li>Package all the dependencies into a docker container which also includes custom Lambda runtime that will be used to execute worker function.</li> <li>Use this container to compile &amp; test your code</li> <li>Zip the compiled function along with any dependencies and upload to an S3 bucket (default bucket name is stored in $S3_LAMBDA_HTCGRID_BUCKET_NAME)<ol> <li>(if S3 bucket is different from $S3_LAMBDA_HTCGRID_BUCKET_NAME) Update HTC-Grid configuration to point to the new location of the target Zip file.</li> </ol> </li> </ol>"},{"location":"api/worker/c%2B%2B/development/","title":"Developing C++ Worker Function","text":"<p>Writing a C++ Worker Function requires creation of an additional shell script: <code>bootstrap</code>. The bootstrap script is a simple wrapper that takes inputs from Agent and passes it to the C++ executable, similarly it takes the response and passes it back to the Agent once task is done. See example below:</p> <p></p> <p>An example of a complete <code>bootstrap</code> can be found here: <code>examples/workloads/c++/mock_computation/bootstrap</code>, although, a custom version of the bootstrap script will be required for the custom Worker function.</p> <ul> <li>The bootstrap script takes task's definition as a string and passes them to the executable as an argument.</li> <li>C++ executable does not need to have a lambda_handler method implemented, instead, the execution starts at the <code>main</code> method.</li> </ul>"},{"location":"api/worker/c%2B%2B/development/#c-including-dependencies","title":"C++ Including Dependencies","text":"<p>Packaging all dependencies and uploading them to an S3 bucket is generally the same as for the Python3 runtime. However, C++ requires an additional step of compiling the source code before zipping all the dependencies. Compiling all the dependencies in the container guarantees that the executable will run in the provided runtime once deployed in HTC-Grid. Refer to a complete example here: <code>examples/workloads/c++/mock_computation/Dockerfile.Build</code></p> <pre><code># Snippet example:\n...\n\nCOPY mock_compute_engine.cpp .\n\nCOPY Makefile .  #Compile the executable in the runtime environment.\n\nRUN make main\n...\n</code></pre>"},{"location":"api/worker/c%2B%2B/development/#3-configuring-htc-grid-deployment","title":"3. Configuring HTC-Grid Deployment","text":"<ul> <li>There are no additional changes required to HTC-Grid to define/launch new Client applications.</li> <li>Some changes might be required to update Worker functions, see below</li> </ul> <p>The root ./Makefile  has 3 options for building and uploading sample Worker functions (e.g., <code>upload-c++</code>, <code>upload-python</code>, and <code>upload-python-ql</code>). These options simply automate all steps described in Section 2 \"Developing a Worker Function\". Follow these examples to build &amp; upload custom worker function code. To execute each option in isolation, specify the option with the make (i.e., instead of happy-path).</p> <pre><code>make upload-c++ TAG=$TAG ACCOUNT_ID=$HTCGRID_ACCOUNT_ID REGION=$HTCGRID_REGION BUCKET_NAME=$S3_LAMBDA_HTCGRID_BUCKET_NAME\n</code></pre> <p>The above steps will upload new Worker Function zip in S3 bucket. However, only new worker pods will be able to benefit from this update. To apply the changes to the entire deployment it is necessary to remove all currently running worker pods (e.g., by executing <code>kubectl delete $(kubectl get po -o name)</code>).</p> <p>Each compute environment pod starts by executing lambda-init container (defined at <code>source/compute_plane/shell/attach-layer</code>) which pulls the Worker function zip package from the S3_LAMBDA_HTCGRID_BUCKET_NAME S3 bucket at the pod boot time.</p>"},{"location":"api/worker/python/development/","title":"Developing Python3 Worker Function","text":"<p>HTC-Grid comes with examples demonstrating how to build Python3 and C++ based Worker functions. The diagram below outlines the relationship between the Agent and the Worker function following Python3 example.</p> <p></p> <p>HTC-Grid's Worker function follows the same API as AWS Lambda function. Following examples supplied with HTC-Grid (<code>examples/workloads/python/mock_computation/mock_compute_engine.py</code> and <code>examples/workloads/python/quant_lib/portfolio_pricing_engine.py</code>) and AWS Lambda documentation write a python module that implements a lambda_handler entry point. This is the function that will be invoked when Agent passes the task to the Worker function.</p> <p>Note, the entry python file and the handler function name can be re-defined in <code>generated/python_runtime_grid_config.json</code> (see below). To apply these changes, <code>terraform apply</code> needs to be re-executed under <code>deployment/grid/terraform</code></p> <pre><code>\"agent_configuration\": {\n\"lambda\": {\n...\n\"lambda_handler_file_name\" : \"portfolio_pricing_engine\",\n\"lambda_handler_function_name\" : \"lambda_handler\"\n}\n}\n</code></pre> <p>The <code>lambda_handler</code> has two arguments an <code>event</code> and a <code>context</code>. - <code>event</code> - is the task's definition that was provided by the client application at the time of the task submission. - <code>context</code> -[To be defined]</p> <p>The return of the <code>lambda_handler</code> function will be treated as the result of the task's execution and will be stored in the Data Plane by the Agent. Subsequently, the Client application will be able to retrieve this output from the Data Plane.</p>"},{"location":"api/worker/python/development/#python3-including-dependencies","title":"Python3 Including Dependencies","text":"<p>HTC-Grid uses custom Lambda runtime lambda-rie which closely mimics the runtime of AWS Lambda. Custom Lambda runtime (along with any additional dependencies) should be included into the docker image to execute the Worker function. See example below:</p> <pre><code># 1. Include custom lambda runtime\nFROM public.ecr.aws/lambda/python:3.8\nRUN yum install -d1 -y  zip\n\n# 2. Create a directory which will hold all relevant dependencies e.g., [/app]\nRUN mkdir -p /app\nWORKDIR /app\n\n# 3. Copy all relevant local dependencies\nCOPY portfolio_pricing_engine.py .\nCOPY american_options.py .\nCOPY european_options.py .\nCOPY ql_common.py .\n\n# 4. Install any third party dependencies into the [/app] folder\nRUN pip install --target=/app QuantLib\n\n# 5. Zip the content of the [/app] which holds all the dependencies at this stage\nRUN mkdir -p /app/build\nRUN zip -yr lambda.zip .\nENTRYPOINT cp lambda.zip /app/build\n</code></pre> <p>The content of the produced zip file should be copied into an S3 bucket. The name of the bucket is stored in S3_LAMBDA_HTCGRID_BUCKET_NAME environmental variable which is set during the deployment stage of the HTC-Grid allowing the grid to retrieve and deploy the correct Worker function package at runtime.</p> <p>For the complete examples please refer to one of the Makefiles in <code>examples/workloads</code> directory.</p>"},{"location":"architecture/architecture/","title":"HTC-Grid Architecture","text":"<p>This document outlines high level architecture and API of HTC-Grid.</p>"},{"location":"architecture/architecture/#definitions","title":"Definitions","text":"<ul> <li> <p>Client Application - A software system that generates job requests and retrieves computation results from the grid system.</p> </li> <li> <p>Task - a unit of work to be scheduled for an execution. A task may have an associated task input and produce an output. The interface of a task takes the same form as the interface of an AWS Lambda handler (Python, Go, Java, C#, etc). </p> </li> <li> <p>Session - a vector of tasks. For example, a job may define a series of scenarios and how they are sub-divided into a set of tasks. Such job can be submitted as a single session containing multiple tasks.</p> </li> <li> <p>Task Input - the set of data which is required in addition to the job definition. Task Input is passed to Engines by reference, bypassing the scheduler itself.</p> </li> <li> <p>The Engine - is a software component responsible for invoking the task execution.</p> </li> </ul>"},{"location":"architecture/architecture/#high-level-architecture","title":"High Level Architecture","text":"<p>This section outlines the high level modular architecture of the cloud native HTC-Grid</p> <p>HTC-Grid has been designed with strong focus on the following tenets: use of cloud native serverless and fully managed services, performance &amp; scalability, availability, cost optimization, and operational simplicity.</p> <p>The grid system is composed of 4 functional components: 1. HTC-Grid\u2019s API provides entry point for Client Applications to interact with the grid, 2. Data Plane facilitates storage, and I/O operations for submitting jobs\u2019 definitions and retrieving computational results, 3. Control Plane (i.e., scheduler) keeps track of the task's execution, grid\u2019s scaling, and error handling 4. A pool of Computing Resources that perform computational tasks.</p> <p>Each component has a clearly defined role and operates strictly within that role. Inter module communication is implemented using standardized AWS API which facilitates independent development and provide further customization options.</p> <p>Internally, each of the 4 functional components (API, Data &amp; Control Planes, and Compute Resources) are built using exclusively cloud native building blocks such as: serverless functions and fully managed services. These blocks require no human maintenance (zero administration), are highly available by design, and can scale horizontally in response to demand.</p> <p></p>"},{"location":"architecture/architecture/#api-interacting-with-htc-grid","title":"API: Interacting with HTC-Grid","text":"<p>Figure below demonstrates high level steps involved in the task submission and result retrieval.</p> <p></p> <p>HTC-Grid allows client applications to submit a session (job) containing a single task, or a vector of tasks. Each submission generates a system-wide unique session_id which is associated with the submission, the session_id is returned to the client application. Successful reception of a session_id indicates that all the tasks of the job are in the system and eventually will be executed.</p> <p>Client applications can use session_id to inquire the state of the tasks within the session (e.g., pending, running, failed, completed, etc.) and subsequently retrieve results once all the tasks of the session are completed. A session is considered to be completed once all tasks of the session are completed. Additionally, if the session did not complete within specified timeout, the session is considered to be failed.</p> <p>During a normal usage, client application would either (i) save the returned session object locally and submit more sessions (jobs) (in case of a batch of jobs) or will wait for the completion of the last submitted session. Note, each session can have a list of tasks associated with it. A multi session submission is also possible, in that case a list of session IDs is returned by the connection object.</p>"},{"location":"architecture/architecture/#control-plane","title":"Control Plane","text":"<p>Control Plane performs the role of a job broker responsible for coordinating and scheduling jobs executions in the grid along with scaling Compute Resources in accordance with demand. Control Plane has built in failure detection and recovery mechanism which allows it to retry and report failed jobs.</p> <p>All building components of the Control Plane are fully managed AWS services (DynamoDB, Simple Queue Service, API Gateway) or serverless functions (i.e., Lambda) which minimizes management and simplifies design.</p>"},{"location":"architecture/architecture/#failure-detection-and-recover","title":"Failure Detection and Recover","text":"<p>Engines acquire tasks by pulling SQS queues, respecting the rank of priority. Once a task has been received by an Engine, the Engine performs an atomic write transaction in DynamoDB to change the status of the task from \u201cpending\u201d to \u201cprocessing\u201d. At this stage a task is associated with that Engine.</p> <p>Failure detection in HTC-Grid is implemented via heart beat mechanism. While the task is being processed, the Engine periodically emits heart-beat messages that update the row corresponding to the task in DynamoDB. These periodic updates indicate to the Control Plane that the Engine is alive and still processing the task.</p> <p>Failure recovery and state reconciliation is implemented using a scheduled Lambda function. This lambda function regularly queries DynamoDB for tasks that are in the processing state but did not receive heart beats from the associated Engines for too long. This indicates that the associated Engines have failed.</p> <p>Depending on the job definition, failed tasks can be retried up to a fixed number of times (by being re-inserted into the queue) or permanently moved into a \u2018failed\u2019 SQS queue for later analysis, following a dead letter queue (https://en.wikipedia.org/wiki/Dead_letter_queue) pattern. All failure events are reported.</p> <p>When the task is completed, the Engine updates DynamoDB for the last time and sets task status to \u201ccompleted\u201d. Afterwards, the Engine tries to acquire a next task from an SQS queue.</p> <p></p>"},{"location":"architecture/architecture/#data-plane","title":"Data Plane","text":"<p>The Data plane is responsible for data distribution across the grid system. Specifically it serves two purposes (i) stores tasks input data associated with jobs definitions (client-to-grid) and (ii) stores results of the computation (grid-to-client).</p> <p>HTC Grid can use S3 or Redis as back-end for the data plane depending on the requirements. Alternatively, existing interface can easily be extended to support other storage systems.</p>"},{"location":"architecture/architecture/#compute-resources","title":"Compute Resources","text":"<p>HTC-Grid utilizes Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod containing two containers an Agent and a Lambda. The Lambda container executes lambda locally within the container (there are no calls made to AWS lambda service, the execution is done within the node Lambda container). The agent provides a connectivity layer between the HTC-Grid and the Lambda container.  The Agent pulls new tasks from the task queues in the Control Plane, once a new task is acquired the agent invokes the Lambda container and passes the task definition. The Lambda container contains custom executable that perform the work. It is responsibility of the Lambda container to connect to the Data Plane and retrieve associated task payload. Once the task is complete, the results is uploaded to the Data Plane. The Grid Agent updates the task\u2019s state to \u201ccompleted\u201d and pulls the next task from the Control Plane.</p> <p></p>"},{"location":"architecture/architecture/#other-functions","title":"Other Functions","text":"<ul> <li> <p>Clients can be called from Step functions to automate complex application dependencies.</p> </li> <li> <p>Multiple instances of the HTC-Grid can be deployed across multiple on the same account and same region or in multiple regions with the client application running either on AWS or on the cloud.</p> </li> </ul>"},{"location":"getting_started/","title":"Introduction","text":"<p>This section steps through the HTC-Grid's AWS infrastructure and software prerequisites. An AWS account is required along with some limited familiarity of AWS services and terraform. The execution of the Happy Path section will create AWS resources not included in the free tier and then will incur cost to your AWS Account. The complete execution of this section will cost at least 50$ per day.</p>"},{"location":"getting_started/configurations/","title":"Configuring Local Environment","text":""},{"location":"getting_started/configurations/#aws-cli","title":"AWS CLI","text":"<p>Configure the AWS CLI to use your AWS account: see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html</p> <p>Check connectivity as follows:</p> <pre><code>$ aws sts get-caller-identity\n{\n\"Account\": \"XXXXXXXXXXXX\",\n    \"UserId\": \"XXXXXXXXXXXXXXXXXXXXX\",\n    \"Arn\": \"arn:aws:iam::XXXXXXXXXXXX:user/XXXXXXX\"\n}\n</code></pre>"},{"location":"getting_started/configurations/#python","title":"Python","text":"<p>The current release of HTC requires python3.7, and the documentation assumes the use of virtualenv. Set this up as follows:</p> <pre><code>$ cd &lt;project_root&gt;/\n$ virtualenv --python=$PATH/python3.7 venv\ncreated virtual environment CPython3.7.10.final.0-64 in 1329ms\n  creator CPython3Posix(dest=&lt;project_roor&gt;/venv, clear=False, no_vcs_ignore=False, global=False)\nseeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/Users/user/Library/Application Support/virtualenv)\nadded seed packages: pip==21.0.1, setuptools==54.1.2, wheel==0.36.2\n  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n</code></pre> <p>Check you have the correct version of python (<code>3.7.x</code>), with a path rooted on <code>&lt;project_root&gt;</code>, then start the environment:</p> <pre><code>$  source ./venv/bin/activate\n(venv) 8c8590cffb8f:htc-grid-0.0.1 $\n</code></pre> <p>Check the python version as follows:</p> <pre><code>$ which python\n&lt;project_root&gt;/venv/bin/python\n$ python -V\nPython 3.7.10\n</code></pre> <p>For further details on virtualenv see https://sourabhbajaj.com/mac-setup/Python/virtualenv.html</p>"},{"location":"getting_started/happy-path/","title":"Happy Path","text":""},{"location":"getting_started/happy-path/#installing-the-htc-grid-software","title":"Installing the HTC-Grid software","text":"<p>Unpack the provided HTC-Grid software ZIP (i.e: <code>htc-grid-0.3.0.tar.gz</code>)  or clone the repository into a local directory of your choice; this directory referred in this documentation as <code>&lt;project_root&gt;</code>. Unless stated otherwise, all paths referenced in this documentation are relative to <code>&lt;project_root&gt;</code>.</p> <p>For first time users or Windows users, we do recommend the use of Cloud9 as the platform to deploy HTC-Grid. The installation process uses Terraform and also make to build up artifacts and environment. This project provides a CloudFormation Cloud9 Stack that installs all the prerequisites-requisites listed above to deploy and develop HTC-Grid. Just follow the standard process in your account and deploy the Cloud9 CloudFormation Stack. Once the CloudFormation Stack has been created, open either the Output section in CloudFormation or go to Cloud9 in your AWS console and open the newly created Cloud9 environment.</p>"},{"location":"getting_started/happy-path/#define-variables-for-deploying-the-infrastructure","title":"Define variables for deploying the infrastructure","text":"<ol> <li>To simplify this installation it is suggested that a unique  name (to be used later) is also used to prefix the different required bucket. TAG needs to follow S3 naming rules.    <pre><code>   export TAG=&lt;Your tag&gt;\n</code></pre> <li> <p>Define the region where the grid will be deployed    <pre><code>   export HTCGRID_REGION=&lt;Your region&gt;\n</code></pre> <code>&lt;Your region&gt;</code> region can be (the list is not exhaustive)</p> <ul> <li><code>eu-west-1</code></li> <li><code>eu-west-2</code></li> <li><code>eu-central-1</code></li> <li><code>us-east-1</code></li> <li><code>us-west-2</code></li> <li><code>ap-northeast-1</code></li> <li><code>ap-southeast-1</code></li> </ul> </li> <li> <p>Define the infrastructure as code tool used for deployment    <pre><code>export IAS=&lt;tool&gt;\n</code></pre> <code>&lt;tool&gt;</code> can take two values:</p> </li> <li><code>cdk</code></li> <li><code>terraform</code></li>"},{"location":"getting_started/happy-path/#create-the-infrastructure-for-storing-the-state-of-the-htc-grid","title":"Create the infrastructure for storing the state of the HTC Grid","text":"<p>The following step creates 3 S3 buckets that will be needed during the installation: * 2 buckets will store the state of the different Terraform deployments (if <code>terraform</code> based deployment) * 1 bucket will store the HTC artifacts (the lambda to be executed by the agent)</p> <pre><code>make init-grid-state  TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre> <p>To validate the creation of the S3 buckets, you can run</p> <pre><code>aws cloudformation describe-stacks --stack-name $TAG --region $HTCGRID_REGION --query 'Stacks[0]'\n</code></pre> <p>That will list the 3 S3 Buckets that we just created.</p>"},{"location":"getting_started/happy-path/#create-and-deploy-htc-grid-images","title":"Create and deploy HTC-Grid images","text":"<p>The HTC-Grid project has external software dependencies that are deployed as container images. Instead of downloading each time from the public DockerHub repository, this step will pull those dependencies and upload into the your Amazon Elastic Container Registry (ECR).</p> <p>Important Note HTC-Grid uses a few open source project with container images stored at DockerHub. DockerHub has a download rate limit policy. This may impact you when running this step as an anonymous user as you can get errors when running the commands below. To overcome those errors, you can re-run the <code>make transfer-images  TAG=$TAG REGION=$HTCGRID_REGION</code> command and wait until the throttling limit is lifted, or optionally you can create an account in hub.docker.com and then use the credentials of the account using <code>docker login</code> locally to avoid anonymous throttling limitations.</p> <ol> <li> <p>As you'll be uploading images to ECR, to avoid timeouts, refresh your ECR authentication token:     <pre><code>make ecr-login REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li> <p>The following command will go to the <code>~/environment/aws-htc-grid/deployment/image_repository/{cdk or terraform}</code> and initialize the  project:     <pre><code>make init-images  TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li> <p>If successful, you can now start the transfer of the images. This can take between 10 and 15 minutes depending on the Internet connection.</p> <p><pre><code>make transfer-images  TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre> The following command will list the repositories You can check which repositories have been created in the ECR console or by executing the command :</p> </li> </ol> <pre><code>aws ecr describe-repositories --query \"repositories[*].repositoryUri\"\n</code></pre> <p>NB: This operation fetches images from external repositories and creates a copy into your ECR account, sometimes the fetch to external repositories may have temporary failures due to the state of the external repositories, If the <code>make transfer-images  TAG=$TAG REGION=$HTCGRID_REGION</code> fails with errors such as the ones below, re-run the command until <code>make transfer-images  TAG=$TAG REGION=$HTCGRID_REGION</code> successfully completes.</p> <pre><code>name unknown: The repository with name 'xxxxxxxxx' does not exist in the registry with id\n</code></pre>"},{"location":"getting_started/happy-path/#build-htc-artifacts","title":"Build HTC artifacts","text":"<p>HTC artifacts include: python packages, docker images, configuration files for HTC and k8s. To build and install these:</p> <ol> <li>Now build the images for the HTC agent. Return to  <code>&lt;project_root&gt;</code>  and run the command:</li> </ol> <pre><code>make happy-path TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre> <pre><code>* If `TAG` is omitted then `mainline` will be the chosen has a default value.\n* If `REGION` is omitted then `eu-west-1` will be used.\n</code></pre> <p>A folder name <code>generated</code> will be created at  <code>&lt;project_root&gt;</code>. This folder should contain the following two files:     * <code>grid_config.json</code> a configuration file for the grid with basic setting     * <code>single-task-test.yaml</code>  the Kubernetes configuration for running a single tasks on the grid.</p>"},{"location":"getting_started/happy-path/#configuring-the-htc-grid-runtime","title":"Configuring the HTC-Grid runtime","text":"<p>The <code>grid_config.json</code> is ready to deploy, but you can tune it before deployment. Some important parameters are: * region : the AWS region where all resources are going to be created. * grid_storage_service : the type of storage used for tasks payloads, configurable between [S3 or Redis] * eks_worker : an array describing the autoscaling  group used by EKS</p>"},{"location":"getting_started/happy-path/#deploying-htc-grid","title":"Deploying HTC-Grid","text":"<p>The deployment time is about 30 min.</p> <ol> <li>Initialize state for the grid    <pre><code>make init-grid-deployment  TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></li> <li>All the dependencies have been created and are now ready. We are now ready to deploy the HTC-Grid project. There is one last thing to note. HTC-Grid deploys a Grafana version behind Amazon Cognito. While you can modify and select which passwords to use in Cognito, the Grafana internal deployment still requires an admin password. Select a memorable password change the value in the placeholder <code>&lt;my_grafana_admin_password&gt;</code> below (make this password follows Cognito default policy):    <pre><code>make apply-custom-runtime  TAG=$TAG REGION=$HTCGRID_REGION GRAFANA_ADMIN_PASSWORD=\n</code></pre></li> </ol> <p>For terraform based deployment, if <code>make apply-custom-runtime</code> is successful then in the <code>deployment/grid/terraform</code> folder two files are  created:</p> <pre><code>* `kubeconfig_htc_$TAG`: this file give access to the EKS cluster through kubectl (example: kubeconfig_htc_aws_my_project)\n* `Agent_config.json`: this file contains all the parameters, so the agent can run in the infrastructure\n</code></pre> <p>For CDK based deployment, if <code>make apply-custom-runtime</code> is successful then please run <pre><code>$(make get-eks-connection  TAG=$TAG REGION=$HTCGRID_REGION)\n</code></pre></p>"},{"location":"getting_started/happy-path/#testing-the-deployment","title":"Testing the deployment","text":"<ol> <li>Testing the Deployment<ol> <li>Get the number of nodes in the cluster using the command below. Note: You should have one or more nodes. If not please the review the configuration files and particularly the variable <code>eks_worker</code> <pre><code>kubectl get nodes\n</code></pre></li> <li>Check is system pods are running using the command below. Note: You should have all pods in running state (this might one minute but no more).    <pre><code>kubectl -n kube-system get po\n</code></pre></li> <li>Check if logging and monitoring is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more).    <pre><code>kubectl -n amazon-cloudwatch get po\n</code></pre></li> <li>Check if metric server is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more).    <pre><code>kubectl -n custom-metrics get po\n</code></pre></li> </ol> </li> </ol>"},{"location":"getting_started/happy-path/#running-the-example-workload","title":"Running the example workload","text":"<p>In the folder mock_computation, you will find the code of the C++ program mocking computation. This program can sleep for a given duration or emulate CPU/memory consumption based on the input parameters. We will use a Kubernetes Jobs to submit  one execution of 1 second of this C++ program. The communication between the job and the grid are implemented by a client in folder ./examples/client/python.</p> <ol> <li>Make sure the connection with the grid is established    <pre><code>kubectl get nodes\n</code></pre>    if an error is returned, please come back to step 2 of the previous section.</li> <li>Change directory to <code>&lt;project_root&gt;</code></li> <li>Run the test:    <pre><code>kubectl apply -f ./generated/single-task-test.yaml\n</code></pre></li> <li> <p>look at the log of the submission:    <pre><code>kubectl logs job/single-task -f\n</code></pre>    The test should take about 3 second to execute.    If you see a successful message without exceptions raised, then the test has been successfully executed.</p> </li> <li> <p>clean the job submission instance:    <pre><code>kubectl delete -f ./generated/single-task-test.yaml\n</code></pre></p> </li> </ol>"},{"location":"getting_started/happy-path/#create-a-cognito-user-cli","title":"Create a Cognito user (CLI)","text":"<p>All the services behind a public URL are protected with an authentication mechanism based on Cognito. So in order to access the Grafana dashboard you will need to create a Cognito user. Please from the root of the project :</p> <ol> <li>Choose a Cognito username:    <pre><code>export USERNAME=&lt;my_cognito_user&gt;\n</code></pre></li> <li>Choose a Cognito password (make this password follows Cognito default policy ) : You can reuse the password chosen  in section Deploy HTC-Grid or create a new one.    <pre><code>export PASSWORD=&lt;my_grafana_admin_password&gt;\n</code></pre></li> <li>Get the client id:    <pre><code>clientid=$(make get-client-id  TAG=$TAG REGION=$HTCGRID_REGION)\n</code></pre></li> <li>Get the userpool id:    <pre><code>userpoolid=$(make get-userpool-id  TAG=$TAG REGION=$HTCGRID_REGION)\n</code></pre></li> <li>Create the user    <pre><code>aws cognito-idp sign-up --region $HTCGRID_REGION --client-id $clientid --username $USERNAME --password $PASSWORD\n</code></pre> The user newly created will be unconfirmed.</li> <li>Confirm user creation:    <pre><code>aws cognito-idp admin-confirm-sign-up --region $HTCGRID_REGION --user-pool-id $userpoolid --username $USERNAME\n</code></pre></li> </ol>"},{"location":"getting_started/happy-path/#accessing-grafana","title":"Accessing Grafana","text":"<p>The HTC-Grid project captures metrics into InfluxDB and exposes those metrics through Grafana. To secure Grafana</p> <ol> <li>To find out the HTTPS endpoint where Grafana has been deployed type:    <pre><code>kubectl -n grafana get ingress | tail -n 1 | awk '{ print \"Grafana URL  -&gt; https://\"$4 }'\n</code></pre>    It should output something like:    <pre><code>Grafana URL  -&gt; https://k8s-grafana-grafanai-XXXXXXXXXXXX-YYYYYYYYYYY.eu-west-2.elb.amazonaws.com\n</code></pre></li> <li>Then take the ADDRESS part and point at that on a browser. Note:It will generate a warning as we are using self-signed certificates. Just accept the self-signed certificate and you will be redirected to a Cognito sign in page. </li> <li>Please enter the username and password created in the previous section.</li> <li>Once you are sign up  with Cognito you will be redirected to the Grafana sign in page. </li> <li> <p>Retrieve the grafana password for the <code>admin</code> user </p> <pre><code>make get-grafana-password TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre> </li> </ol>"},{"location":"getting_started/happy-path/#uninstalling-and-destroying-htc-grid","title":"Uninstalling and destroying HTC grid","text":"<p>The destruction time is about 15 min.</p> <ol> <li> <p>To remove the grid resources run the following command:    <pre><code>make destroy-custom-runtime TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li> <p>To remove the images from the ECR repository execute</p> <ol> <li> <p>For cdk based deployment please run the following command:   <pre><code>image_list=\"\nnode-exporter\namazonlinux\nk8s-cloudwatch-adapter\namazon/cloudwatch-agent\nprometheus\naws-for-fluent-bit\nlambda-build\nkube-state-metrics\ninfluxdb\ngrafana\nk8s-sidecar\nlambda\nconfigmap-reload\nbusybox\ncurl\npushgateway\namazon/aws-node-termination-handler\nalertmanager\ncluster-autoscaler\naws-xray-daemon\nawshpc-lambda\nlambda-init\nsubmitter\n\"\n</code></pre></p> </li> <li> <p>And then   <pre><code>echo $image_list | tr ' ' '\\n'  |  xargs -L1  aws ecr delete-repository  --region $HTCGRID_REGION --force --repository-name </code></pre></p> </li> </ol> </li> <li> <p>For all deployments          <pre><code>make destroy-images TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li>Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command    <pre><code>make delete-grid-state TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></li> </ol>"},{"location":"getting_started/happy-path/#build-the-documentation","title":"Build the documentation","text":"<ol> <li>Go at the root of the git repository</li> <li>run the following command     <pre><code>make doc\n</code></pre>    or for deploying the server :     <pre><code>make serve\n</code></pre></li> </ol>"},{"location":"getting_started/portfolio_example/","title":"Portfolio example","text":""},{"location":"getting_started/portfolio_example/#portfolio-evaluation-using-quantlib","title":"Portfolio Evaluation using QuantLib","text":"<p>This example demonstrates the use of the QuantLib with its SWIG Python bindings.</p> <p>Components: - /examples/workloads/python/quant_lib/portfolio_pricing_client.py client application that generates a portfolio of trades using simple portfolio generator. Trades then being split into individual tasks and sent to the HTC-Grid for computation. Once all tasks are completed, the client application merges results together to determine total value of the portfolio.</p> <ul> <li>/examples/workloads/python/quant_lib/portfolio_pricing_engine.py compute engine that receives a list of trades to evaluate (could be entire portfolio or just a single trade). The engine uses QuantLib to evaluate the value of the portfolio.</li> </ul>"},{"location":"getting_started/portfolio_example/#deployment","title":"Deployment","text":"<ol> <li>Follow all the steps in the main Readme file until you reach section \"Build HTC artifacts\". In this section you will need to modify the <code>make</code> command replacing the <code>happy-path</code> with the <code>python-quant-lib-path</code> as follows:     <pre><code>make python-quant-lib-path TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre> This will apply the following changes:</li> <li>prepare python runtime environment for the lambda functions</li> <li> <p>generate 2 sample yaml files that will be used to deploy testing client containers.</p> </li> <li> <p>After <code>make</code> is completed, please run     <pre><code>make apply-python-runtime  TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> </ol> <p>Follow all the remaining steps as is in the main readme file.</p>"},{"location":"getting_started/portfolio_example/#running-the-example","title":"Running the example","text":"<p>Two default configurations are provided. The first configuration submits a portfolio containing a single trade.</p> <pre><code>kubectl apply -f ./generated/portfolio-pricing-single-trade.yaml\n</code></pre> <p>The second configuration submits a portfolio containing multiple trades.</p> <pre><code>kubectl apply -f ./generated/portfolio-pricing-book.yaml\n</code></pre> <p>Refer to the corresponding yaml files to change the configuration of the client application and refer to the help of the client application to identify all options.</p>"},{"location":"getting_started/portfolio_example/#uninstalling-installing-and-destroying-htc-grid","title":"Uninstalling-Installing and destroying HTC grid","text":"<p>The destruction time is about 15 min.</p> <ol> <li> <p>To remove the grid resources run the following command:    <pre><code>make destroy-python-runtime TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li> <p>To remove the images from the ECR repository execute    <pre><code>make destroy-images TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></p> </li> <li>Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command    <pre><code>make delete-grid-state TAG=$TAG REGION=$HTCGRID_REGION\n</code></pre></li> </ol>"},{"location":"getting_started/prerequisite/","title":"Software Prerequisites","text":"<p>The following resources should be installed upon you local machine (Linux and macOS only are supported).</p> <ul> <li> <p>docker version &gt; 1.19</p> </li> <li> <p>kubectl version &gt; 1.21 (usually installed alongside Docker)</p> </li> <li> <p>python 3.7</p> </li> <li> <p>virtualenv</p> </li> <li> <p>AWS CLI version 2</p> </li> <li> <p>helm version &gt; 3</p> </li> <li> <p>JQ</p> </li> </ul> <p>For terraform based deployment, please consider the installation of the following tool: * terraform v0.13.4 or terraform v0.14.9</p> <p>For CDK based deployment, please consider the installation of the following tools: * Node.js version 16.x or higher * NPM usually installed with Node.js * CDK version 2.x</p>"},{"location":"user_guide/configuring_priority_queues/","title":"Configuring Priority Queues","text":"<p>By default HTC-Grid comes configured with a single task queue implemented via SQS. However, HTC-Grid also supports task prioritization which is implemented suing multiple SQS queues. In such configuration each SQS queue corresponds to a specific priority. At runtime Agents attempt to pull tasks from the higher priority queues before checking queues containing lower priority.</p> <p>To enable multiple priorities the following 3 steps need to be configured prior to HTC-Grid deployment</p> <ol> <li> <p>in ''deployment/grid/terraform/control_plane/sqs.tf'' modify variable priorities to have sufficient number of priorities that are required. Follow the same naming/numbering convention as outlined below     <pre><code># Default configuration with 1 priority\nvariable \"priorities\" {\n    default     = {\n        \"__0\" = 0\n    }\n}\n...\n# Example configuration with 3 priorities\nvariable \"priorities\" {\n    default     = {\n        \"__0\" = 0\n        \"__1\" = 1\n        \"__2\" = 2\n    }\n}\n</code></pre></p> </li> <li> <p>Configure GRID_CONFIG file (e.g., ''python_runtime_grid_config.json'') before deploying HTC-Grid. Note this file is auto-generated from the corresponding .tpl file located in ''examples/configurations/'' hence re-running ''make'' can overwrite modifications, consider updating .tpl file instead.</p> <p><pre><code>\"task_queue_service\": \"PrioritySQS\",\n\"task_queue_config\": \"{'priorities':3}\",\n</code></pre> Set ''task_queue_service'' to PrioritySQS indicating that multiple priorities are used. Then, update ''task_queue_config'' to contain the appropriate number of priorities created in step 1.</p> </li> </ol>"},{"location":"user_guide/creating_your_a_client/","title":"Configuring Client and Sending Tasks to the HTC Grid","text":"<p>We assume that all previous steps have been successfully completed, there is at least one pod that is running in the system and thus can execute tasks. Furthermore, we consider that client application will be running on an EC2 instance.</p> <ol> <li> <p>Setup a Cloud9 (EC2) Instance     a) Go to the AWS console in the Cloud9 service     b) Click on \"Create your environment\"     c) Select your favorite OS and machine type (at least t3.small)     d) In network settings, please used the VPC id and, the subnet that was created as part of your infrastructure deployment (look for your unique suffix in the VPC/subnet pages of the AWS console).</p> <ul> <li>Go to the AWS console  in the VPC service.</li> <li>Find the VPC id where the EKS cluster</li> <li>Find a public subnet id attached to the VPC where the EKS cluster is running. (The public subnet should have an Internet Gateways and should be able to assign IPs)</li> </ul> <p>e) Click on create and wait for the validation</p> </li> <li> <p>Checkout the same version of the repository as was used to deploy infrastructure</p> </li> <li> <p>Upload the infrastructure settings to the Cloud9 instance so that client knows ALB endpoints and connects to the right instance of the HTC grid.     a) On the machine that was used for deployment. Go in ./infrastructure/ folder where <code>terraform apply</code> has been run.</p> <pre><code>terraform output agent_config\n</code></pre> <p>c) copy the produced <code>Agent_config.json</code> file into cloud9 and note the location d) set environment variable <code>export AGENT_CONFIG_FILE=/&lt;path&gt;/Agent_config.json</code> e) set environment variable <code>export INTRA_VPC=1</code> This will allow client to send tasks to ALB without authentication through Cognito as it is deployed in the same VPC as the grid. For clients running from outside the VPC an additional authentication step is required.</p> </li> <li> <p>From the root folder execute:</p> <pre><code>make packages\n\n#make sure that these two files are created:\nls ./dist/\napi-0.1-py3-none-any.whl  utils-0.1-py3-none-any.whl\n</code></pre> <p>If these files are not created or if it is based on python 2, then run virtualenv as described at the start of the document</p> </li> <li> <p>Install clients requirements on Cloud9</p> <pre><code>cd ./examples/client/python/\npip3 install -r requirements.txt\n</code></pre> </li> <li> <p>Sample client and workload generator is located here <code>./examples/client/python/client.py</code>. Read help and browse through the code to be able to submit tasks and sessions, see examples below:</p> <p>To show the example client application help</p> <pre><code>python3 ./client.py --help\n</code></pre> <p>To submits a single session (containing a single task by default)</p> <pre><code>python3 ./client.py  --njobs 1\n</code></pre> <p>To submits 2 batches of 4 sessions each, where each session contains 3 tasks. Total 4*3*2 24 tasks.</p> <p><pre><code>python3 ./client.py  --njobs 2 --job_size 3 --job_batch_size 4\n</code></pre> To tarts 5 threads, each submits a single session with 1 job with a custom arguments to the executable.</p> <pre><code>python3 ./client.py  --njobs 1 --worker_arguments \"5000 1 100\" -nthreads 5\n</code></pre> </li> </ol>"},{"location":"user_guide/htcgrid_management/","title":"Managing HTC-Grid after deployment","text":""},{"location":"user_guide/htcgrid_management/#managing-the-htc-grid-agent","title":"Managing the HTC-Grid agent","text":"<ol> <li>Make connection with EKS, go in the deployment/grid/terraform folder where terraform apply has been run. Then make sure that KUBECONFIG is set (see step 6).</li> <li> <p>Set up the storage for helm     <pre><code>export HELM_DRIVER=configmap\n</code></pre></p> </li> <li> <p>Deploy the agent</p> <pre><code>helm install &lt;your release name&gt; ../charts/agent-htc-lambda --set fullnameOverride=htc-agent\n</code></pre> </li> <li> <p>Test the installation by running</p> <pre><code>helm test &lt;your release name&gt;\n</code></pre> </li> <li> <p>Delete the agent</p> <pre><code>helm uninstall &lt;your release name&gt;\n</code></pre> </li> <li> <p>Get deployed pods ( see notes after agent deployment )</p> </li> <li>Get the log of a pod: (  see notes after agent deployment )</li> <li>Describe the state of a pod , useful for debugging situation never run, (see notes after agent deployment)</li> <li> <p>Execute a command into a pod</p> <pre><code>kubectl exec  &lt;pod name&gt; -c &lt;container name&gt; &lt;command&gt;\n</code></pre> </li> <li> <p>Open an interactive session into a pod</p> <pre><code>kubectl exec -it &lt;pod name&gt; bash\n</code></pre> </li> <li> <p>Get config-map used in the pods, i.e., current configuration</p> <pre><code>kubectl get cm agent-configmap -o yaml\n</code></pre> </li> <li> <p>Get information about the pod autoscaler</p> <pre><code>kubectl get hpa\nkubectl get hpa -w\n</code></pre> </li> <li> <p>Updating number of replicas (my_cluster is your release name that you selected with helm)</p> </li> </ol> <pre><code>helm upgrade --set replicaCount=5 --set foo=newbar my_cluster ./agent-htc-lambda\n</code></pre>"},{"location":"user_guide/htcgrid_management/#common-commands","title":"Common commands","text":"<ul> <li> <p>Get logs from a running agent and lambda worker</p> <pre><code>kubectl logs -c &lt;agent or lambda&gt; &lt;pod name&gt;\n\nExamples:\nkubectl logs -c agent htc-agent-544bd95456-wgzqs\n\nkubectl logs -c lambda htc-agent-544bd95456-wgzqs\n</code></pre> </li> <li> <p>Launching Tests</p> <pre><code>cd root/examples/submissions/k8s_jobs\nkubectl apply -f &lt;test-name.yaml&gt;\n\nexample:\nkubectl apply -f scaling-test.yaml\n\nFollow the execution of the test\nkubectl logs job/scaling-test -f\n</code></pre> </li> <li> <p>Deleting all running pods</p> <pre><code>kubectl delete $(kubectl get po -o name)\n</code></pre> </li> </ul>"},{"location":"user_guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user_guide/troubleshooting/#troubleshooting-htc-grid","title":"Troubleshooting HTC-Grid","text":"<p>This section captures some of the errors we have captured in the past and how to resolve them</p>"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply","title":"Error on terraform apply:","text":"<pre><code>terraform apply -var-file ./../src/eks/Agent_config_mainline.json\"\n...\n...\n\nError: DeleteConflict: Certificate: XXXXXXXXXXX is currently in use by arn:aws:elasticloadbalancing:eu-west-1:XXXXXXXXX:loadbalancer/app/k8s-grafana-grafanai-026d965437/c04519ef28804b31. Please remove it first before deleting it from IAM.\nstatus code: 409, request id: 9e621094-2dba-44ac-967d-c764470c1474\n</code></pre>"},{"location":"user_guide/troubleshooting/#resolution","title":"Resolution:","text":"<pre><code>kubectl -n grafana get ingress\n\nkubectl -n grafana delete ingress grafana-ingress\n\nterraform apply -var-file ./../src/eks/Agent_config_mainline.json\"\n</code></pre>"},{"location":"user_guide/troubleshooting/#error-from-terraform-apply-when-pulling-the-images","title":"Error from terraform Apply when pulling the images","text":"<pre><code>Error: Error running command 'if ! docker pull curlimages/curl:7.73.0\nthen\n  echo \"cannot download image curlimages/curl:7.73.0\"\n  exit 1\nfi\nif ! docker tag curlimages/curl:7.73.0 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\nthen\n  echo \"cannot tag curlimages/curl:7.73.0 to 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\"\n  exit 1\nfi\nif ! docker push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\nthen\n  echo \"echo cannot push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\"\n  exit 1\nfi\n': exit status 1. Output: 7.73.0: Pulling from curlimages/curl\n</code></pre>"},{"location":"user_guide/troubleshooting/#resolution_1","title":"Resolution","text":"<p>Rerun the terraform command, DockerHub has throttling limits that may cause spurious errors like this</p>"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply_1","title":"Error on terraform apply:","text":"<pre><code>Error: cannot re-use a name that is still in use\n\n  on resources/influxd.tf line 19, in resource \"helm_release\" \"influxdb\":\n  19: resource \"helm_release\" \"influxdb\" {\n</code></pre>"},{"location":"user_guide/troubleshooting/#resolution_2","title":"Resolution:","text":"<pre><code>export HELM_DRIVER=configmap\nhelm list -n influxdb\nhelm -n influxdb uninstall influxdb\n...\n&lt;restart tarraform apply&gt;\n</code></pre>"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply_2","title":"Error on terraform apply:","text":"<pre><code>Error: error reading VPC Endpoint Service (com.amazonaws.eu-north-1.elasticloadbalancing): InvalidServiceName: The Vpc Endpoint Service 'com.amazonaws.eu-north-1.elasticloadbalancing' does not exist\n    status code: 400, request id: 60127863-944c-467b-983b-8f8b79f332c0\n\n  on .terraform/modules/vpc.vpc/vpc-endpoints.tf line 656, in data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\":\n 656: data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\" {\n</code></pre>"},{"location":"user_guide/troubleshooting/#resolution_3","title":"Resolution","text":"<p>Some AWS regions currently don't have VPC Endpoint Services available for certain services used by HTC-Grid. This means that at this stage HTC-Grid can not be deployed in these regions. Below is the list of tested regions where we encountered this issue: * eu-north-1</p>"}]}